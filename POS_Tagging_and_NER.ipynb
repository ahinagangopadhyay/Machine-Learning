{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWiYGuAGqVXjK2v3owpZEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahinagangopadhyay/Machine-Learning/blob/main/POS_Tagging_and_NER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "revJjObwarbx",
        "outputId": "b2f6b3e8-2a2f-43b0-cd49-23fc22b02be5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatGPT --> NNP\n",
            "is --> VBZ\n",
            "an --> DT\n",
            "AI --> NNP\n",
            "developed --> VBN\n",
            "by --> IN\n",
            "OpenAI --> NNP\n",
            ". --> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "# Force re-download of correct resources\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', force=True)\n",
        "\n",
        "# Use Treebank tokenizer (avoids buggy punkt_tab issue)\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"ChatGPT is an AI developed by OpenAI.\"\n",
        "\n",
        "# Tokenize and POS tag\n",
        "tokens = tokenizer.tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "# Output\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word} --> {tag}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WwEZVIL9iMuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9zJJV6jfp5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lukjxOnCbpHb",
        "outputId": "38e2958e-ff34-48d6-aea0-b8de3ca2a12d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "text = \"The quick brown fox jumps over the lazy dog near the riverbank.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word} --> {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTe13y3Gfqk3",
        "outputId": "3588783d-bac7-437e-c81e-c134fb3291fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The --> DT\n",
            "quick --> JJ\n",
            "brown --> NN\n",
            "fox --> NN\n",
            "jumps --> VBZ\n",
            "over --> IN\n",
            "the --> DT\n",
            "lazy --> JJ\n",
            "dog --> NN\n",
            "near --> IN\n",
            "the --> DT\n",
            "riverbank --> NN\n",
            ". --> .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy if not already installed\n",
        "!pip install -q spacy\n",
        "\n",
        "# Download English model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Python code\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"Barack Obama was born in Hawaii and served as the President of the United States.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract and print named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} --> {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EriNZmt3iNnh",
        "outputId": "95fb8395-31eb-4f42-ef2c-55d26cb3aa2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Barack Obama --> PERSON\n",
            "Hawaii --> GPE\n",
            "the United States --> GPE\n"
          ]
        }
      ]
    }
  ]
}