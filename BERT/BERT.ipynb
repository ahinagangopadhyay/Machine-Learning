{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21e0bf4a-b0ea-4d1a-9069-c6a163afbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16bbad03-c411-408b-8aab-24ab06ad3e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "                                                text  label\n",
      "0                      I loved the new Batman movie!      1\n",
      "1               The plot was boring and predictable.      0\n",
      "2  A masterpiece. The acting, direction—everythin...      1\n",
      "3         I wouldn’t recommend this movie to anyone.      0\n",
      "4  Decent movie with a strong performance by the ...      1\n",
      "\n",
      "Total samples: 15\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Custom Dataset CSV\n",
    "df = pd.read_csv(\"custom_sentiment_dataset.csv\")\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee243336-c99c-423e-858e-bbf99fce55c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 12\n",
      "Validation samples: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Split Data into Training and Validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bd6f63d-e0ce-4729-85c6-17d0720d800a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHINA\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing training data...\n",
      "Tokenizing validation data...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load BERT tokenizer and tokenize data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"\\nTokenizing training data...\")\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee4c165-58f5-4bfd-b123-1dbd70891f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datasets ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Convert tokenized data to TensorFlow Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\n",
    "print(\"\\nDatasets ready for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec2f8be-d9c0-4921-9138-0585cc49af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AHINA\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Load pretrained BERT model for sequence classification\n",
    "print(\"Loading BERT model...\")\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "babcee2e-264d-445b-aaa8-db09f5374ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled.\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print(\"Model compiled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9acf743-7fda-4dae-a479-76ca31ddd5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 70s 9s/step - loss: 0.6614 - accuracy: 0.5833 - val_loss: 0.7773 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6042 - accuracy: 0.8333 - val_loss: 0.8002 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.6089 - accuracy: 0.8333 - val_loss: 0.8373 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.5256 - accuracy: 0.9167 - val_loss: 0.7774 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.5265 - accuracy: 0.9167 - val_loss: 0.7172 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.5156 - accuracy: 0.8333 - val_loss: 0.6734 - val_accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.4290 - accuracy: 1.0000 - val_loss: 0.6476 - val_accuracy: 0.6667\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 2s 988ms/step - loss: 0.4442 - accuracy: 1.0000 - val_loss: 0.6397 - val_accuracy: 0.6667\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 2s 893ms/step - loss: 0.3834 - accuracy: 1.0000 - val_loss: 0.6394 - val_accuracy: 0.6667\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 2s 975ms/step - loss: 0.3590 - accuracy: 1.0000 - val_loss: 0.6404 - val_accuracy: 0.6667\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train the model\n",
    "print(\"\\nStarting training...\")\n",
    "history = model.fit(\n",
    "    train_dataset.shuffle(100).batch(8),\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    validation_data=val_dataset.batch(8)\n",
    ")\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "726c1eab-2e77-4216-83a8-3ee18c70bf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: A truly positive story told beautifully.\n",
      "Predicted sentiment: Positive (60.57%)\n",
      "\n",
      "Review: This was the worst film I've seen in years. Completely boring and predictable.\n",
      "Predicted sentiment: Negative (74.54%)\n",
      "\n",
      "Review: It was not great but not terrible either.\n",
      "Predicted sentiment: Negative (75.68%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Sentiment Analysis on New Reviews\n",
    "\n",
    "# Sample new reviews\n",
    "new_reviews = [\n",
    "    \"A truly positive story told beautifully.\",\n",
    "    \"This was the worst film I've seen in years. Completely boring and predictable.\",\n",
    "    \"It was not great but not terrible either.\"\n",
    "]\n",
    "\n",
    "# Tokenize new reviews\n",
    "new_encodings = tokenizer(new_reviews, truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Predict with the model (returns logits)\n",
    "outputs = model(new_encodings)\n",
    "\n",
    "# Convert logits to probabilities (softmax)\n",
    "probs = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "\n",
    "# Get predicted class (0 or 1)\n",
    "predicted_classes = tf.argmax(probs, axis=1).numpy()\n",
    "\n",
    "# Print predictions\n",
    "for review, pred, prob in zip(new_reviews, predicted_classes, probs.numpy()):\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = prob[pred] * 100\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Predicted sentiment: {sentiment} ({confidence:.2f}%)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ffe7596-13a2-46f6-886f-27d59b12b7ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 123\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis movie was fantastic, I really enjoyed it!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m     ]\n\u001b[0;32m    121\u001b[0m }\n\u001b[1;32m--> 123\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV file created at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd33eee-c4b5-4ab7-85d4-ba9b58926a24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
